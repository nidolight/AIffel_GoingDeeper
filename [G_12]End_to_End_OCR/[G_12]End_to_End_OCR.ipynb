{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ce3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_ocr.detection import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce000f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
    "# os.chdir(path)\n",
    "# print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa375f7",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "데이터셋으로는 Symas에서 만든 LMDB를 사용합니다.\n",
    "LMDB: Lightning Memory-Mapped Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/ocr/data/MJ/MJ_train\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
    "\n",
    "TRAIN_DATA_PATH = HOME_DIR+'/data/MJ/MJ_train'\n",
    "VALID_DATA_PATH = HOME_DIR+'/data/MJ/MJ_valid'\n",
    "TEST_DATA_PATH = HOME_DIR+'/data/MJ/MJ_test'\n",
    "\n",
    "print(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814af5f",
   "metadata": {},
   "source": [
    "lmdb를 통해 훈련데이터셋의 이미지를 4개만 열어서 실제 shape가 어떻게 생겼는지, 이미지나 라벨은 어떻게 달려 있는지를 확인해 보도록 합시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d998fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:72, height:31\n",
      "target_img_size:(74, 32)\n",
      "display img shape:(74, 32, 3)\n",
      "label:Lube\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=74x32 at 0x7FDAA941B580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:82, height:31\n",
      "target_img_size:(84, 32)\n",
      "display img shape:(84, 32, 3)\n",
      "label:Spencerian\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x32 at 0x7FDAA98A4D60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:115, height:31\n",
      "target_img_size:(100, 32)\n",
      "display img shape:(100, 32, 3)\n",
      "label:accommodatingly\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x32 at 0x7FDAA941B670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:140, height:31\n",
      "target_img_size:(100, 32)\n",
      "display img shape:(100, 32, 3)\n",
      "label:CARPENTER\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAgCAIAAABrSUp5AAAQZ0lEQVR4nIVaWXMaR9udfQUxwyohJJCDZEdxIieppMpV+TPx78xNKheJHbsir9qFjSR2GMHA7DP9XRyr3zEo+eZCBU1P97Oe5zzdYp89e8Z8+RBCWJblOI4QQgihg/jAsiw+cBxHf4rjGPOTJGFZls5hWRaLYHKSJBjkOC49E3NWX0zvyLJsHMfprwzDYIQ+WJkumJZ2Sbv01yRJOI6johJCeJ5nWRbSchwXxzFkE5iVB2slSZLWM60G9MRakInnefyU1p/Ohxwcx0mSBA2jKEqvSRXAptSm6U2prZeEWTIEzIfdsemqvf7NvlRlJuXmdLgsG4s6ma5FN07LR7/Sh2pI90jbIi0EZgqCsGSs9LurgZbeIq3qqvzpD+lF7n3SkSUIAnZJxzsi635jLW2TTodV+eicOI6pdVZVpU8URSzL8jwvCEIQBEzK9PT1JRMvRTSzEuNLW/y3ae7VdAk90k5dSqxlY0EZJhUONJ/T0tBkoUsjz+keVPO04dKYlU6QJXxZDd4lO/6HRf5jkdVxaCcIAtSh1kkHSnqve4xFIyVtWiaVLDzPY+koirCrIAhIqziO8S5djVoc1iSEhGHo+74oikuK0clpc9PxtE1Xw5A+6QCneLSkIH7CakmSIMaZOy9S1676e9lYyZfPkqx4X5ZlWCqOY8yJokhVVRgCxkorxtyFHipmFEVRFPm+v+TntAL3Bsv/O7iazqvT0lWeEBLHcRAEiHS4nM6kXlcURRAEjuPuAXjf96m4FO2wDT6HYUgjC+O0uiVJsmosEAUAFrwnCEK6IC4hIy1PzJf5khZyNcvSg6s5jsF0qaEhr2kaRZV0IaamD4KAECIgXQFSCP7pdCrLsqqqi8VC1/UHDx7k8/k4jieTyc3NzXw+h54Ios3NzXK5fHp6ijChlKJQKCiKkiRJGIaiKE6nU9d1eZ6PoihJEp7nM5mMLMuEEMhhWZZhGJiDh2XZKIo8z1tbW0uSRFXV6XRKeRaWwkzHcURRjKIom83CJdls1rZtWGqxWGQyGY7jZFmeTCbYFNFNLcLzfBzHiqIsFgusyTCM4ziIKfg+iiJRFAXqfAxJkkQImc1mzWbz4cOHpVIJs+v1uqqqb9++XSwWCNpcLvf48WNJks7OzkDboIkgCJVKpVarMQwjSVKSJPP5/PT0dDgcRlGkaVo2m93f35dlmWEYnudns9nt7e3Z2RnDMLIsN5tN0zQZhoEjEYYXFxfdbjefz+/v7yuK4nleq9UajUYMw/z444+GYciyPJvNXrx4sba2dnBwkMlkJpMJx3G6rsdxHIahbdsfPnwghDSbzUKhAEIAZAjDsN/vn5+f67peqVTK5bKiKMDlfr9/fX29WCw+YxaMhfQBmrAsu7e39/PPP7Ms2+12fd+vVCqGYYRhGASBKIqiKHqet7m5WalUptMpUpLGLeIil8vFcTwejzVNq1arjuNYloXUk2W5UqnAk0mS5HK5crl8dXXlui7LsoZh1Ot113WREVBpNpshlwuFQqFQ8Dyv2+3iJ1EUy+WyJEnT6TQIgnK5XK1W4zgWRVEQBEmS5vP52tra2dkZUmdzc9MwDHiChmq32w2CQJKkra2tRqMB1Nd1PZPJeJ63WCw+4wMtQBSzG43G06dPHcf5448/Xrx48fr1a8uyZrNZr9dDWyMIgqIojUYjCILZbIYAgbFgbtu24zi2bfv58+cfPnwIgsA0TWCk7/uO49ze3tq2fXR09OrVKxgin8/j116v5zhOr9d7/vz5y5cvwzAcj8eTyUSSJMdxqJ8FQeB5nhByfX0dhuHJycnbt28FQVhfXx+Px7///jvy6OXLl+/fvweM+L6P+UEQDAaD58+f//3334vFwvf9Tqejqqrv+5ZlxXFsWdY///xzfn6ey+VqtRpFfY4WIIQrQj0MwxcvXlxeXtq2jfwfjUZAhyRJPM+r1+umaSZJslgsCCGUBwCGYLvpdOo4zmw2AyjEcSzLMjgEx3Gu6378+LHf79u2DaSjDkuSZDQatdvt4XDIcRxCEus7jsNxnCiK+Xweb8myLAgC8LRSqeRyuXa7/fHjR0Rxp9OZTqdhGM7nc0CNZVmKoti23Wq12u22qqqDwcD3fVSeXq8XhqFlWa1Wq9VqEUI0TQOKMQzD0SoG6K1Wq6ZpnpycDAaDTCbDsqymaa7rnp6eosQyDKNp2sOHD4fDIZAVWEsLGeCMENJutzmOK5VKoigOBgN0FTzPq6pqmqbjOFEUcRy3trbGsuxsNqMCcBwHgBME4ezsrN1uIyVFUSwWi8D+YrGIxKxUKq7rjkYjTdMURel2u91u1zAMRVHG43EQBEmSwJQoC9VqlWXZq6srKD4YDM7OzgghKC/ZbFYUxdFoBCaYJMl4PKau+lxiwZg0TdvY2IiiqNPpMAwTBEEYhlEUHR8fX19f06K5s7OjqurFxQVyO81gYUrDMOI4Hg6Hsixvbm5altXr9Wi11jSNYZibmxuUm0wm0+v1UBYVRSmVSo7juK4rSRLLssfHx7Zti6IYx7GqqpVKZTAYOI6TzWZ1Xec4LpPJLBYLz/OiKOp2u2/fvrUsCyE2mUw8z5vNZsfHxzQkAXmorTzPf/jw4fb2lmVZmGZrayuKovF4LMvyxsZGkiTdbpc2QwLqAoJW07R8Pt/v9+fzOSHE8zxFUYIg8DyPMoZMJlOr1Vqtlud5lLjS+hrHsa7rpmlGUaTr+t7eXi6Xe/fu3Ww2C8MQWAO87/V6qCRhGF5dXYVhiOIliiIh5OHDhxzHBUHw7t27JEnQHiiKIsvy9fX1N998A+gER/n06ROsCYbI83wul0uSBHgax/FoNJIkKYoiWZYB/F9//TUS5fXr15T98TxfKBRs255Op1tbW6VSaTQajcdjStkEGlmEkHK5zPP8aDSCgXieRw5qmkbTrVaraZrGcdz29vYSM4ThTNNUFMX3/W+//bZarZ6fn7fbbWQcx3GKouTzeYZhms2mLMtfffXV8fHxYDDACqZpIoJ2d3dlWT45OUHuAFOKxeJisRiNRmEYKoqiqiq4dbvdliQJFRDsR1VVx3EGgwHl5fCrYRi6rhNCHj16xPN8u93med73fYCsaZrA0/39/Xq97nne0dFREAQoTYQQgVIkGAsEHaaRZRlwCycIgpDNZhuNBpIL/A3BTBtDQsj6+nocx7e3t7VabTabXV5eep6n6zqNO0j/3XffhWHYarWOj48RdCzLlkol13XPz89RYS8uLiAeCFetVptOp+PxGMGFCGUYBpQYxTGOY+CA53nz+Rz5xTAMfN9oNAghnU7HcRxBEK6urmj/yLJsuVxGpTo4OLAs6+TkpN/vw3ygGkIcxzC8KIroEoIgAA8GlMLGlmWBcKmqen19bdt2uVwGvYaZ4HyGYQzDWCwWR0dHmUwGVIOSdZZlNzY2eJ6/vLwMgmA+n8Mcuq67rquqqmEY8/n89evXgAVJkiRJWiwWoihmMhld1zudThiGg8Fgf3+/WCz6vg9qClrkeR6ytVAovH//HqRPEATbtuFXANzR0dFwOAT9xpEkZCuVSr7vX11dNRqN29vbm5sbZAPYXJIkHO1v2dSxHCZB8ydPnpRKpSAIwK16vd5ff/315s2bfr9PCJEkKQxDHCTwPG+apqZpg8EAlahQKMCavu/DZMVicTabHR4evnr1CkRRlmXHccBjQcTR0BQKhd3dXfQZDMPkcjmGYcbjsa7riCbgfbvdBnoCTERRzGazkiR1u13YwvM8KKjrOsuy8/ncsixJkmRZRpYgqHVdNwyj1+sdHh6GYVir1TKZTLo1xmocEBRIL0mSYRiiKCLC6/U6z/PAjkePHuVyufPzc9/3YRoULFQZ+AedhGVZlBOhzUQYK4qiKEq/3/d9H+/STg28NIqiq6srgPTe3l6z2YTnoyjK5XKoU0EQOI5j27Ysy/ATOmGctHAcVy6Xbdu2LAvu8TxPFEXf90ulUi6XG4/HyErTNH/44Qee51HQ0TP5vh9FEdg83OP7PiYwOM8KwxAIh/4jn8/XarUgCIrF4u7u7qdPn5Ik2d7e3tzcHI/HYRjqui7LcjabhdtlWQZyFQqFSqUiCAJiwbZtz/N2dnZub29d1+U47smTJ4ZhDIfDXC5n2zY0FEVR1/VsNlssFuE21LhSqdTv90GVTdOkQcowjOu6tm2DRsAxKCmiKBqGUSwWbdtGaQJhhm+y2SzIMMD3wYMHoPXwaLFYDIIAlAUHTfSUAVj8+YgG5SwIgk6nUygU1tfXVVUVBEGWZc/zTk9PS6XSTz/9BPP/8ssvb968efz4calUQu0fDofdbleW5adPnyIqDcNgGGY6nWqaVq/Xp9Pp5eXl9vZ2s9n0fX9ra2s8Hs9mM0EQMpmM67qiKO7u7tZqtTiODw4OUAFkWf7zzz9RZw4ODjY2NgghGxsbNzc3QRBASfSDOE1TVXVnZ2d9fV1RFNS7w8NDWB/LVqtV5JdhGKZp1uv13377LYoiRVGazeb29rYkSSAu4/G4UqnU6/Uoivr9Pjz9uZEWBCEMQ0LIcDgE1cRpgeu67XZ7MBg8fvx4MpnAppCSEAKhcUgCXg7oDYIAvaFlWZZloZFiGGZjY2M2m6EqjUYj2sSiDK2vr2OQvbso63a74He5XE7TtOl0iixDsQZOjcfjbDY7n89d111fX3/w4AHgXBAEVVURI6hL29vbiLV8Pr+5uQnSj9BDcZdlGQSN5/lOp1Ov12u1muu6nz59Incnq+yzZ8/QweL4CSd5mqaFYQjIlCTJdd0gCFRVBYuBmTFZEAQgaDabRbCgB4RNcR7Q6XREUaxWqxcXF7RPRLZi31KpRAixbTubzYLXrK2tTSaT+XwOaFcUBfKg7xVF0TTNnZ2dw8NDuB2EFucNqFFovyFno9HIZDKgF6j7uq5fX1+3Wi0Uwb29PQiP0yRd17///nuWZXE0BPhnGIb99ddf6YEBkAy5ihIAz9NOm717mC8feqhIUqe69CgdytCmgdxdygJo8Jle09I5zMoNZvp4A6cOlE9izfSdK/ZFgwkOhBxCH0JPyiA87f5o34MRWvqQoQJdGmwCeJ82hCiKcAi7ci1GH9CitKD4AKJIbwTAY6EYdGDvTpmZlcNi8uUFEuU0gHy0E/gJLCd9GE1dSE3A3d1EpEMBD3d3+4sHYUFPsQGI+Pq/m06wr/TdJxUxLfpqWDH/csUAj3meBwwWBAEHAwBRcCK8Cz/RowuSOiNP70gJMHxOGSNVjNJD+lA+gbdg33TerGqU5lb4QLcQ0vGCkF4yBL3koIGQrFxtUj3pflAGra+maWm6C0tBAvyla5LUJcWSMjSd6bEfbYMoMqwGPtoXaibuX+5A06/QSKcO+1/u01Hm7pZl6b8t6Fpc6r8nlmS6dwQuokGOlanfsEsaj1b/LnmOIiP35VUmkwrtJUnuXeHf1qdqcnf3QGzqGjBJEiGtPH5O350xKYynr63usTRCyyUOfxBl2AgnJLT+MqnAWUKTVbXT17dp76aFX4rxe02z9MpqGlKzQmz6VaCupha5F4DoC/caawkpEEGoWRTI6ZUiTi9hGlr70nVgKTvSAJ8uuEvicXe3k2lJKLCki2zaDavGolWVpPplzPw/1Dl1/YcR3BsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x32 at 0x7FDAA941B7F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# env에 데이터를 불러올게요\n",
    "# lmdb에서 데이터를 불러올 때 env라는 변수명을 사용하는게 일반적이에요\n",
    "env = lmdb.open(TRAIN_DATA_PATH, \n",
    "                max_readers=32, \n",
    "                readonly=True, \n",
    "                lock=False, \n",
    "                readahead=False, \n",
    "                meminit=False)\n",
    "\n",
    "# 불러온 데이터를 txn(transaction)이라는 변수를 통해 엽니다\n",
    "# 이제 txn변수를 통해 직접 데이터에 접근 할 수 있어요\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        # index를 이용해서 라벨 키와 이미지 키를 만들면\n",
    "        # txn에서 라벨과 이미지를 읽어올 수 있어요\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # 이미지는 버퍼를 통해 읽어오기 때문에 \n",
    "        # 버퍼에서 이미지로 변환하는 과정이 다시 필요해요\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "\n",
    "        # 원본 이미지 크기를 출력해 봅니다\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        # 이미지 비율을 유지하면서 높이를 32로 바꿀거에요\n",
    "        # 하지만 너비를 100보다는 작게하고 싶어요\n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32)        \n",
    "        print('target_img_size:{}'.format(target_img_size))        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "\n",
    "        # 이제 높이가 32로 일정한 이미지와 라벨을 함께 출력할 수 있어요       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7fba0",
   "metadata": {},
   "source": [
    "### input 이미지\n",
    "lmdb를 활용하여 케라스 모델 학습용 `MJSynth`데이터셋 클래스를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06529356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MJDatasetSequence(Sequence):\n",
    "    # 객체를 초기화 할 때 lmdb를 열어 env에 준비해둡니다\n",
    "    # 또, lmdb에 있는 데이터 수를 미리 파악해둡니다\n",
    "    def __init__(self, \n",
    "                 dataset_path,\n",
    "                 label_converter,\n",
    "                 batch_size=1,\n",
    "                 img_size=(100,32),\n",
    "                 max_text_len=22,\n",
    "                 is_train=False,\n",
    "                 character='') :\n",
    "        \n",
    "        self.label_converter = label_converter\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.character = character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    # index에 해당하는 image와 label을 읽어옵니다\n",
    "    # 위에서 사용한 코드와 매우 유사합니다\n",
    "    # label을 조금 더 다듬는 것이 약간 다릅니다\n",
    "    def _get_img_label(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width, self.img_size[1])\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            # label을 약간 더 다듬습니다\n",
    "            label = label.upper()\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "            label = label[:self.max_text_len]\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    # __getitem__은 약속되어있는 메서드입니다\n",
    "    # 이 부분을 작성하면 slice할 수 있습니다\n",
    "    # 자세히 알고 싶다면 아래 문서를 참고하세요\n",
    "    # https://docs.python.org/3/reference/datamodel.html#object.__getitem__\n",
    "    # \n",
    "    # 1. idx에 해당하는 index_list만큼 데이터를 불러\n",
    "    # 2. image와 label을 불러오고 \n",
    "    # 3. 사용하기 좋은 inputs과 outputs형태로 반환합니다\n",
    "    def __getitem__(self, idx):\n",
    "        # 1.\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64') * self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        # 2.\n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            # 인코딩 과정에서 '-'이 추가되면 max_text_len보다 길어질 수 있어요\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "        \n",
    "        # 3.\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff6f9f",
   "metadata": {},
   "source": [
    "`_get_img_label()` \n",
    "- 이미지 데이터를 img, label의 쌍으로 가져옵니다.\n",
    "\n",
    "- `model.fit()`에서 호출되는 `__getitem__()` 메소드에서 배치 단위만큼 가져온 데이터셋을 리턴합니다.  \n",
    "\n",
    "- 다양한 사이즈의 이미지를 모두 height는 32로 맞추고, width는 최대 100까지로 맞추게끔 가공해 줍니다.\n",
    "\n",
    "### Label encoding\n",
    "준비한 데이터를 모델에 학습하기에 적절한 형태로 인코딩해주어야 합니다.\n",
    "`LabelConverter` 클래스를 통해 각 Character를 class로 생각하고 이를 step에 따른 class index로 변환해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483c62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelConverter(object):\n",
    "\n",
    "    def __init__(self, character):\n",
    "        self.character = \"-\" + character\n",
    "        self.label_map = dict()\n",
    "        for i, char in enumerate(self.character):\n",
    "            self.label_map[char] = i\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_label = []\n",
    "        for i, char in enumerate(text):\n",
    "            if i > 0 and char == text[i - 1]:\n",
    "                encoded_label.append(0)    # 같은 문자 사이에 공백 문자 label을 삽입\n",
    "            encoded_label.append(self.label_map[char])\n",
    "        return np.array(encoded_label)\n",
    "\n",
    "    def decode(self, encoded_label):\n",
    "        target_characters = list(self.character)\n",
    "        decoded_label = \"\"\n",
    "        for encode in encoded_label:\n",
    "            decoded_label += self.character[encode]\n",
    "        return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda8b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of characters is 36\n"
     ]
    }
   ],
   "source": [
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3696e",
   "metadata": {},
   "source": [
    "LavelConverter가 잘 작동하는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006f285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encdoded_text:  [ 8  5 12  0 12 15]\n",
      "Decoded_text:  HEL-LO\n"
     ]
    }
   ],
   "source": [
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c5f25",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101babd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7c3e9",
   "metadata": {},
   "source": [
    "`build_crnn_model()` : `K.ctc_batch_cost()`를 활용하여, `image_input`을 입력으로, 마지막 Label을 'output'이라는 이름으로 출력하는 레이어를 갖도록 모델을 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18066f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "    \n",
    "    # Build CRNN model\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(\n",
    "        inputs=model_input,\n",
    "        outputs=loss_out\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd10882",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "앞에서 정의한 MJDatasetSequence로 데이터를 적절히 분리하여 구성된 데이터셋을 통해 학습을 시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ce2183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 모델을 준비합니다\n",
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "model = build_crnn_model()\n",
    "\n",
    "# 모델을 컴파일 합니다\n",
    "optimizer = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbe9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "106/565 [====>.........................] - ETA: 1:08 - loss: 28.2028"
     ]
    }
   ],
   "source": [
    "# 훈련이 빨리 끝날 수 있도록 ModelCheckPoint와 EarlyStopping을 사용합니다\n",
    "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min'\n",
    ")\n",
    "model.fit(train_set,\n",
    "          steps_per_epoch=len(train_set),\n",
    "          epochs=25,\n",
    "          validation_data=val_set,\n",
    "          validation_steps=len(val_set),\n",
    "          callbacks=[ckp, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561fe93",
   "metadata": {},
   "source": [
    "미리 학습된 모델을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 다음은 학습된 모델의 가중치가 저장된 경로입니다\n",
    "# checkpoint_path = HOME_DIR + '/data/model_checkpoint.hdf5'\n",
    "\n",
    "# # 데이터셋과 모델을 불러옵니다\n",
    "# test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "# model = build_crnn_model()\n",
    "# model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9d72f",
   "metadata": {},
   "source": [
    "inference 전용 모델을 생성해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364060cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crnn 모델은 입력이 복잡한 구조이므로 그대로 사용할 수가 없습니다\n",
    "# 그래서 crnn 모델의 입력중 'input_image' 부분만 사용한 모델을 새로 만듭니다.\n",
    "input_data = model.get_layer('input_image').output\n",
    "y_pred = model.get_layer('output').output\n",
    "model_pred = Model(inputs=input_data, outputs=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f774a60",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339df87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델이 inference한 결과를 글자로 바꿔주는 역할을 합니다\n",
    "# 코드 하나하나를 이해하기는 조금 어려울 수 있습니다\n",
    "def decode_predict_ctc(out, chars = TARGET_CHARACTERS):\n",
    "    results = []\n",
    "    indexes = K.get_value(\n",
    "        K.ctc_decode(\n",
    "            out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
    "            greedy=False , beam_width=5, top_paths=1\n",
    "        )[0][0]\n",
    "    )[0]\n",
    "    text = \"\"\n",
    "    for index in indexes:\n",
    "        text = text+chars[index] if index != -1 else text\n",
    "\n",
    "    return text\n",
    "\n",
    "# 모델과 데이터셋이 주어지면 inference를 수행합니다\n",
    "# index개 만큼의 데이터를 읽어 모델로 inference를 수행하고\n",
    "# 결과를 디코딩해 출력해줍니다\n",
    "def check_inference(model, dataset, index = 5):\n",
    "    for i in range(index):\n",
    "        inputs, outputs = dataset[i]\n",
    "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
    "        output = model.predict(img) \n",
    "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS).replace('-','')\n",
    "        print(\"Result: \\t\",result)\n",
    "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_inference(model_pred, test_set, index=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d01f0",
   "metadata": {},
   "source": [
    "## End-to-End OCR\n",
    "`detect_text()` : 이미지 경로를 받아 해당 이미지 내의 문자를 찾아내줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab28663",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IMG_PATH = os.path.join(HOME_DIR, 'sample.jpg')\n",
    "\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f334ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img_path):\n",
    "    # 배치 크기를 위해서 dimension을 확장해주고 kera-ocr의 입력 차원에 맞게 H,W,C로 변경합니다.\n",
    "    # 배치의 첫 번째 결과만 가져옵니다.\n",
    "    # 시각화를 위해서 x와 y좌표를 변경해줍니다. (앞선 h dimension으로 인해 y,x로 표기됨)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result_img = img.copy()\n",
    "    result_img = result_img.astype('uint8')\n",
    "    input_img = img[tf.newaxis,...]\n",
    "    result = detector.detect(input_img)\n",
    "    ocr_result = result[0]\n",
    "    \n",
    "    cropped_imgs = []\n",
    "    for text_result in ocr_result:\n",
    "        x_min = int(text_result[:,0].min() - 5)\n",
    "        x_max = int(text_result[:,0].max() + 5)\n",
    "        y_min = int(text_result[:,1].min() - 5)\n",
    "        y_max = int(text_result[:,1].max() + 5)\n",
    "        cropped_imgs.append(img[y_min:y_max,x_min:x_max])\n",
    "        \n",
    "        for i in range(4):\n",
    "            if i<3:\n",
    "                result_img = cv2.line(result_img,tuple(map(int,text_result[i])),tuple(map(int,text_result[i+1])),(255,0,0),3)\n",
    "            else:\n",
    "                result_img = cv2.line(result_img,tuple(map(int,text_result[3])),tuple(map(int,text_result[0])),(255,0,0),3)\n",
    "    return result_img, cropped_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb7564",
   "metadata": {},
   "source": [
    "inference한 후에는 이를 시각화하여 잘라진 단어의 영역을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil, cropped_img = detect_text(SAMPLE_IMG_PATH)\n",
    "display(Image.fromarray(img_pil))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6c484",
   "metadata": {},
   "source": [
    "`recognize_img()` : 위에서 얻은 단어 이미지에서 Recognition model로 text를 인식해주어 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_img(input_img, input_img_size=(100,32)):\n",
    "    # 잘려진 단어 이미지를 인식하는 코드\n",
    "    img = input_img[:,:,::-1]\n",
    "    img = cv2.resize(img, input_img_size)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result_img = img.copy()\n",
    "    img = img.transpose(1,0,2)\n",
    "    img = img[tf.newaxis,...]\n",
    "    output = model_pred.predict(img)\n",
    "    result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS).replace('-','')\n",
    "    print(\"Result: \\t\", result)\n",
    "    display(Image.fromarray(result_img.astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _img in cropped_img:\n",
    "    recognize_img(_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70d096",
   "metadata": {},
   "source": [
    "end to end 함수를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b665fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end2end(img_path):\n",
    "    img_pil, cropped_img = detect_text(img_path)\n",
    "    display(Image.fromarray(img_pil))\n",
    "    for _img in cropped_img:\n",
    "        recognize_img(_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(HOME_DIR, 'sample2.jpg')\n",
    "end2end(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0323b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(HOME_DIR, 'sample3.jpg')\n",
    "end2end(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250cd8ad",
   "metadata": {},
   "source": [
    "기울어진 문자나 영어, 한국어에 대해서는 인식을 못하는 것을 확인할 수 있습니다.\n",
    "- 기울어진 문자의 경우 deskew와 선형변환을 통해 보정한다면 결과가 좋을 것 같습니다\n",
    "- 영어가 아닌 경우에 대해서는 모델을 추가로 학습시켜야 될 것 같습니다.\n",
    "\n",
    "---\n",
    "## 회고\n",
    "예전 ocr을 주제로 프로젝트를 했던 경험이 있는데 이번 going deeper 노드를 진행하면서 리마인드할 수 있어서 좋았습니다. cv2가 아닌 PIL의 Image를 사용하는데 익숙치 않아 어려움이 있었지만 해당 노드를 통해 Image의 사용법에 대해 좀 더 익숙해질 수 있었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
